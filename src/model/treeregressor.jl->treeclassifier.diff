--- /home/gio/.julia/dev/DecisionTree/src/model/treeregressor.jl	Mon Jan  3 05:08:07 2022
+++ /home/gio/.julia/dev/DecisionTree/src/model/treeclassifier.jl	Mon Jan  3 05:08:41 2022
@@ -4,18 +4,19 @@
 
 # written by Poom Chiarawongse <eight1911@gmail.com>
 
-module treeregressor
+module treeclassifier
 	
 	import DecisionTree: fit
 
 	using ..ModalLogic
 	using ..DecisionTree
 	using DecisionTree.util
-	const L = DecisionTree.RegressionLabel
+	const L = DecisionTree.ClassificationLabel
 	using Logging: @logmsg
 	import Random
 	import StatsBase
 	using StructuredArrays # , FillArrays # TODO choose one
+
 
 	mutable struct NodeMeta{U}
 		region             :: UnitRange{Int}                   # a slice of the samples used to decide the split of the node
@@ -58,12 +59,13 @@
 	struct Tree{S}
 		root           :: NodeMeta{Float64}
 		list           :: Vector{S}
+		labels         :: Vector{L}
 		initConditions :: Vector{<:DecisionTree._initCondition}
 	end
-
 	# Find an optimal local split satisfying the given constraints
 	#  (e.g. max_depth, min_samples_leaf, etc.)
 	# TODO move this function inside the caller function, and get rid of all parameters
+	# Also TODO : is it possible to compile the function once loss_function is known? (maybe as a generated function) 
 	Base.@propagate_inbounds function _split!(
 		node                  :: NodeMeta{<:AbstractFloat}, # the node to split
 		####################
@@ -83,10 +85,11 @@
 		allowRelationGlob     :: Vector{Bool},
 		####################
 		indX                  :: AbstractVector{<:Integer},   # an array of sample indices (we split using samples in indX[node.region])
+		n_classes             :: Int,
 		####################
 		_perform_consistency_check :: Union{Val{true},Val{false}},
 		####################
-		writing_lock          :: Threads.Condition,
+		# writing_lock          :: Threads.Condition,
 		####################
 		rng                   :: Random.AbstractRNG,
 	) where {U}
@@ -98,6 +101,7 @@
 
 		# Gather all values needed for the current set of instances
 		# TODO also slice the dataset?
+
 		@inbounds Yf = Y[indX[region]]
 		@inbounds Wf = W[indX[region]]
 
@@ -111,30 +115,16 @@
 		############################################################################
 		# Prepare counts
 		############################################################################
-		sums, (tsum, nt),
+		(nc, nt),
 		(node.purity, node.label) = begin
-			sums = [Wf[i]*Yf[i]       for i in 1:n_instances]
-			# ssqs = [Wf[i]*Yf[i]*Yf[i] for i in 1:n_instances]
-			
-			# tssq = zero(U)
-			# tssq = sum(ssqs)
-			# tsum = zero(U)
-			tsum = sum(sums)
-			# nt = zero(U)
-			nt = sum(Wf)
-			# @inbounds @simd for i in 1:n_instances
-			# 	# tssq += Wf[i]*Yf[i]*Yf[i]
-			# 	# tsum += Wf[i]*Yf[i]
-			# 	nt += Wf[i]
-			# end
-
-			# purity = (tsum * label) # TODO use loss function
-			# purity = tsum * tsum # TODO use loss function
-			# tmean = tsum/nt
-			# purity = -((tssq - 2*tmean*tsum + (tmean^2*nt)) / (nt-1)) # TODO use loss function
-			purity = -(loss_function(sums, nt))
-			label =  tsum / nt # Assign the most likely label before the split
-			sums, (tsum, nt), (purity, label)
+			nc = fill(zero(U), n_classes)
+			@inbounds @simd for i in 1:n_instances
+				nc[Yf[i]] += Wf[i]
+			end
+			nt = sum(nc)
+			purity = -(loss_function(nc, nt))
+			label = argmax(nc) # Assign the most likely label before the split
+			(nc, nt), (purity, label)
 		end
 		############################################################################
 		############################################################################
@@ -146,22 +136,24 @@
 		# Preemptive leaf conditions
 		############################################################################
 		if (
+			# If all the instances belong to the same class, make this a leaf
+			    (nc[node.label]       == nt)
 			# No binary split can honor min_samples_leaf if there aren't as many as
 			#  min_samples_leaf*2 instances in the first place
-			    (min_samples_leaf * 2 >  n_instances)
-		  # equivalent to old_purity > -1e-7
-			 # || (tsum * node.label    > -1e-7 * nt + tssq)
+			 || (min_samples_leaf * 2 >  n_instances)
+			# If the node is pure enough, avoid splitting # TODO rename purity to loss
+			 || (node.purity          >= max_purity_at_leaf)
 			# Honor maximum depth constraint
 			 || (max_depth            <= node.depth))
 			node.is_leaf = true
-			@logmsg DTDetail "leaf created: " (min_samples_leaf * 2 >  n_instances) (tsum * node.label    > -1e-7 * nt + tssq) (tsum * node.label) (-1e-7 * nt + tssq) (max_depth <= node.depth)
+			@logmsg DTDetail "leaf created: " (min_samples_leaf * 2 >  n_instances) (nc[node.label] == nt) (node.purity  >= max_purity_at_leaf) (max_depth <= node.depth)
 			return
 		end
 		############################################################################
 		############################################################################
 		############################################################################
 
-		# TODO try this solution for rsums and lsums
+		# TODO try this solution for rsums and lsums (regression case)
 		# rsums = Vector{U}(undef, n_instances)
 		# lsums = Vector{U}(undef, n_instances)
 		# @simd for i in 1:n_instances
@@ -259,16 +251,10 @@
 				########################################################################
 				# Apply decision to all instances
 				########################################################################
-				(rsums, nr, lsums, nl, rsum, lsum), consistency_sat_check = begin
-					# Initialize right counts
-					# rssq = zero(U)
-					rsum = zero(U)
-					nr   = zero(U)
-					# TODO experiment with running mean instead, because this may cause a lot of memory inefficiency
-					# https://it.wikipedia.org/wiki/Algoritmi_per_il_calcolo_della_varianza
-					rsums = Float64[] # Vector{U}(undef, n_instances)
-					lsums = Float64[] # Vector{U}(undef, n_instances)
-
+				(ncr, nr, ncl, nl), consistency_sat_check = begin
+					# Re-initialize right counts
+					nr = zero(U)
+					ncr = fill(zero(U), n_classes)
 					if isa(_perform_consistency_check,Val{true})
 						consistency_sat_check .= 1
 					end
@@ -279,50 +265,32 @@
 						
 						# TODO make this satisfied a fuzzy value
 						if !satisfied
-							push!(rsums, sums[i_instance])
-							# rsums[i_instance] = sums[i_instance]
-							nr   += Wf[i_instance]
-							rsum += sums[i_instance]
-							# rssq += ssqs[i_instance]
+							nr += Wf[i_instance]
+							ncr[Yf[i_instance]] += Wf[i_instance]
 						else
-							push!(lsums, sums[i_instance])
-							# lsums[i_instance] = sums[i_instance]
 							if isa(_perform_consistency_check,Val{true})
 								consistency_sat_check[i_instance] = 0
 							end
 						end
 					end
-
 					# Calculate left counts
-					# lsum = tsum - rsum
-					# lssq = tssq - rssq
-					nl   = nt - nr
+					ncl = Vector{U}(undef, n_classes)
+					ncl .= nc .- ncr
+					nl = nt - nr
 					
-					(rsums, nr, lsums, nl, rsum, lsum), consistency_sat_check
-				end
-
-				@logmsg DTDebug "  (n_left,n_right) = ($nl,$nr)"
-
+					(ncr, nr, ncl, nl), consistency_sat_check
+				end
+				
 				########################################################################
 				########################################################################
 				########################################################################
 				
+				@logmsg DTDebug "  (n_left,n_right) = ($nl,$nr)"
+
 				# Honor min_samples_leaf
 				if nl >= min_samples_leaf && (n_instances - nl) >= min_samples_leaf
 					purity_times_nt = begin
-						- (nl * loss_function(lsums, nl) + nr * loss_function(rsums, nr))
-											# TODO use loss_function instead
-											# ORIGINAL: TODO understand how it works
-											# purity_times_nt = (rsum * rsum / nr) + (lsum * lsum / nl)
-											# Variance with ssqs
-											# purity_times_nt = (rmean, lmean = rsum/nr, lsum/nl; - (nr * (rssq - 2*rmean*rsum + (rmean^2*nr)) / (nr-1) + (nl * (lssq - 2*lmean*lsum + (lmean^2*nl)) / (nl-1))))
-											# Variance
-											# var = (x)->sum((x.-StatsBase.mean(x)).^2) / (length(x)-1)
-											# purity_times_nt = - (nr * var(rsums)) + nl * var(lsums))
-											# Simil-variance that is easier to compute but it doesn't work with few samples on the leaves
-											# var = (x)->sum((x.-StatsBase.mean(x)).^2)
-											# purity_times_nt = - (var(rsums) + var(lsums))
-											# println("purity_times_nt: $(purity_times_nt)")
+						- (nl * loss_function(ncl, nl) + nr * loss_function(ncr, nr))
 					end
 					if purity_times_nt > best_purity_times_nt # && !isapprox(purity_times_nt, best_purity_times_nt)
 						#################################
@@ -358,17 +326,16 @@
 		# println("best_purity_times_nt / nt - node.purity = $(best_purity_times_nt / nt - node.purity)")
 		# println("min_purity_increase * nt =  $(min_purity_increase) * $(nt) = $(min_purity_increase * nt)")
 
-		# @logmsg DTOverview "purity_times_nt increase" best_purity_times_nt/nt node.purity (best_purity_times_nt + node.purity) (best_purity_times_nt - node.purity)
+		# @logmsg DTOverview "purity_times_nt increase" best_purity_times_nt/nt node.purity (best_purity_times_nt/nt + node.purity) (best_purity_times_nt/nt - node.purity)
 		# If the best split is good, partition and split accordingly
 		@inbounds if (
 			##########################################################################
 			##########################################################################
 			##########################################################################
-			best_purity_times_nt == typemin(Float64)
-									# || best_purity_times_nt - tsum * node.label <= min_purity_increase * nt # ORIGINAL
-									|| (best_purity_times_nt / nt - node.purity <= min_purity_increase * nt)
+				best_purity_times_nt == typemin(Float64)
+									|| (best_purity_times_nt/nt - node.purity <= min_purity_increase)
 								)
-			@logmsg DTDebug " Leaf" best_purity_times_nt tsum node.label min_purity_increase nt (best_purity_times_nt / nt - tsum * node.label) (min_purity_increase * nt)
+			@logmsg DTDebug " Leaf" best_purity_times_nt min_purity_increase (best_purity_times_nt/nt) node.purity ((best_purity_times_nt/nt) - node.purity)
 			##########################################################################
 			##########################################################################
 			##########################################################################
@@ -376,45 +343,50 @@
 			return
 		else
 			best_purity = best_purity_times_nt/nt
-			
+
 			# Compute new world sets (= take a modal step)
 
 			# println(decision_str)
 			decision_str = display_decision(best_i_frame, best_relation, best_feature, best_test_operator, best_threshold)
-
+			
 			# TODO instead of using memory, here, just use two opposite indices and perform substitutions. indj = n_instances
 			unsatisfied_flags = fill(1, n_instances)
+			if isa(_perform_consistency_check,Val{true})
+				world_refs = []
+			end
 			for i_instance in 1:n_instances
 				# TODO perform step with an OntologicalModalDataset
 
-				# instance = ModalLogic.getInstance(X, node.i_frame, indX[i_instance + r_start])
-				X = get_frame(Xs, node.i_frame)
-				Sf = Sfs[node.i_frame]
+				# instance = ModalLogic.getInstance(X, best_i_frame, indX[i_instance + r_start])
+				X = get_frame(Xs, best_i_frame)
+				Sf = Sfs[best_i_frame]
 				# instance = ModalLogic.getInstance(X, indX[i_instance + r_start])
 
 				# println(instance)
 				# println(Sf[i_instance])
-				_sat, _ss = ModalLogic.modal_step(X, indX[i_instance + r_start], Sf[i_instance], node.relation, node.feature, node.test_operator, node.threshold)
-				Threads.lock(writing_lock)
-				(satisfied,Ss[node.i_frame][indX[i_instance + r_start]]) = _sat, _ss
-				Threads.unlock(writing_lock)
-				@logmsg DTDetail " [$satisfied] Instance $(i_instance)/$(n_instances)" Sf[i_instance] (if satisfied Ss[node.i_frame][indX[i_instance + r_start]] end)
+				_sat, _ss = ModalLogic.modal_step(X, indX[i_instance + r_start], Sf[i_instance], best_relation, best_feature, best_test_operator, best_threshold)
+				# Threads.lock(writing_lock)
+				(satisfied,Ss[best_i_frame][indX[i_instance + r_start]]) = _sat, _ss
+				# Threads.unlock(writing_lock)
+				@logmsg DTDetail " [$satisfied] Instance $(i_instance)/$(n_instances)" Sf[i_instance] (if satisfied Ss[best_i_frame][indX[i_instance + r_start]] end)
 				# println(satisfied)
-				# println(Ss[node.i_frame][indX[i_instance + r_start]])
+				# println(Ss[best_i_frame][indX[i_instance + r_start]])
 				# readline()
 
 				# I'm using unsatisfied because sorting puts YES instances first, but TODO use the inverse sorting and use satisfied flag instead
 				unsatisfied_flags[i_instance] = !satisfied
-			end
-
-			@logmsg DTOverview " Branch ($(sum(unsatisfied_flags))+$(n_instances-sum(unsatisfied_flags))=$(n_instances) samples) on frame $(node.i_frame) with decision: $(decision_str), purity $(node.purity)"
+				if isa(_perform_consistency_check,Val{true})
+					push!(world_refs, _ss)
+				end
+			end
 
 			@logmsg DTDetail " unsatisfied_flags" unsatisfied_flags
 
 			if length(unique(unsatisfied_flags)) == 1
 				throw_n_log("An uninformative split was reached. Something's off\nPurity: $(node.purity)\nSplit: $(decision_str)\nUnsatisfied flags: $(unsatisfied_flags)")
 			end
-			
+			@logmsg DTOverview " Branch ($(sum(unsatisfied_flags))+$(n_instances-sum(unsatisfied_flags))=$(n_instances) samples) on frame $(best_i_frame) with decision: $(decision_str), purity $(best_purity)"
+
 			# Check consistency
 			consistency = if isa(_perform_consistency_check,Val{true})
 					unsatisfied_flags
@@ -423,8 +395,14 @@
 			end
 
 			if best_consistency != consistency
-				errStr = "Something's wrong with the optimization steps for relation $(node.relation), feature $(node.feature) and test_operator $(node.test_operator).\n"
-				errStr *= "Branch ($(sum(unsatisfied_flags))+$(n_instances-sum(unsatisfied_flags))=$(n_instances) samples) on frame $(node.i_frame) with decision: $(decision_str), purity $(best_purity)\n"
+				errStr = "Something's wrong with the optimization steps."
+				errStr *= "Relation $(best_relation), feature $(best_feature) and test_operator $(best_test_operator).\n"
+				errStr *= "Possible causes:\n"
+				errStr *= "- feature returning NaNs\n"
+				errStr *= "- erroneous enumAccReprAggr for relation $(best_relation), aggregator $(ModalLogic.existential_aggregator(best_test_operator)) and feature $(best_feature)\n"
+				errStr *= "\n"
+				errStr *= "Branch ($(sum(unsatisfied_flags))+$(n_instances-sum(unsatisfied_flags))=$(n_instances) samples) on frame $(best_i_frame) with decision: $(decision_str), purity $(best_purity)\n"
+				errStr *= "$(length(indX[region])) Instances: $(indX[region])\n"
 				errStr *= "Different partition was expected:\n"
 				if isa(_perform_consistency_check,Val{true})
 					errStr *= "Actual: $(consistency) ($(sum(consistency)))\n"
@@ -437,27 +415,60 @@
 					diff = best_consistency-consistency
 					errStr *= "Difference: $(diff)\n"
 				end
+				errStr *= "unsatisfied_flags = $(unsatisfied_flags)\n"
+
+				if isa(_perform_consistency_check,Val{true})
+					errStr *= "world_refs = $(world_refs)\n"
+					errStr *= "new world_refs = $([Ss[best_i_frame][indX[i_instance + r_start]] for i_instance in 1:n_instances])\n"
+				end
 				
 				# for i in 1:n_instances
-					# errStr *= "$(ModalLogic.getChannel(Xs, indX[i + r_start], best_feature))\t$(Sf[i])\t$(!(unsatisfied_flags[i]==1))\t$(Ss[node.i_frame][indX[i + r_start]])\n";
+					# errStr *= "$(ModalLogic.getChannel(Xs, indX[i + r_start], best_feature))\t$(Sf[i])\t$(!(unsatisfied_flags[i]==1))\t$(Ss[best_i_frame][indX[i + r_start]])\n";
 				# end
 
-				throw_n_log("ERROR! " * errStr)
-			end
-
-			@logmsg DTDetail "pre-partition" region indX[region] unsatisfied_flags
-			node.split_at = util.partition!(indX, unsatisfied_flags, 0, region)
-			@logmsg DTDetail "post-partition" indX[region] node.split_at
-
-			# For debug:
-			# indX = rand(1:10, 10)
-			# unsatisfied_flags = rand([1,0], 10)
-			# partition!(indX, unsatisfied_flags, 0, 1:10)
-			
-			# Sort [Xf, Yf, Wf, Sf and indX] by Xf
-			# util.q_bi_sort!(unsatisfied_flags, indX, 1, n_instances, r_start)
-			# node.split_at = searchsortedfirst(unsatisfied_flags, true)
-		end
+				# throw_n_log("ERROR! " * errStr)
+				println("ERROR! " * errStr) # TODO fix
+			end
+
+			@logmsg DTDetail " unsatisfied_flags" unsatisfied_flags
+
+			# TODO this should be satisfied, since min_samples_leaf is always > 0 and nl,nr>min_samples_leaf
+			if length(unique(unsatisfied_flags)) == 1
+				errStr = "An uninformative split was reached. Something's off\n"
+				errStr *= "Purity: $(best_purity)\n"
+				errStr *= "Split: $(decision_str)\n"
+				errStr *= "Unsatisfied flags: $(unsatisfied_flags)"
+
+				println("ERROR! " * errStr) # TODO fix
+				# throw_n_log(errStr)
+				node.is_leaf = true
+				return
+			else
+				# split the samples into two parts:
+				#  ones for which the is satisfied and those for whom it's not
+				node.purity         = best_purity
+				node.i_frame        = best_i_frame
+				node.relation       = best_relation
+				node.feature        = best_feature
+				node.test_operator  = best_test_operator
+				node.threshold      = best_threshold
+				
+
+				@logmsg DTDetail "pre-partition" region indX[region] unsatisfied_flags
+				node.split_at = util.partition!(indX, unsatisfied_flags, 0, region)
+				@logmsg DTDetail "post-partition" indX[region] node.split_at
+
+				# For debug:
+				# indX = rand(1:10, 10)
+				# unsatisfied_flags = rand([1,0], 10)
+				# partition!(indX, unsatisfied_flags, 0, 1:10)
+				
+				# Sort [Xf, Yf, Wf, Sf and indX] by Xf
+				# util.q_bi_sort!(unsatisfied_flags, indX, 1, n_instances, r_start)
+				# node.split_at = searchsortedfirst(unsatisfied_flags, true)
+			end
+		end
+
 		# println("END split!")
 		# readline()
 	end
@@ -531,6 +542,16 @@
 		elseif min_samples_leaf < 1
 			throw_n_log("min_samples_leaf must be a positive integer "
 				* "(given $(min_samples_leaf))")
+		# if loss_function in [util.entropy]
+		# 	max_purity_at_leaf_thresh = 0.75 # min_purity_increase 0.01
+		# 	min_purity_increase_thresh = 0.5
+		# 	if (max_purity_at_leaf >= max_purity_at_leaf_thresh)
+		# 		println("Warning! It is advised to use max_purity_at_leaf<$(max_purity_at_leaf_thresh) with loss $(loss_function)"
+		# 			* "(given $(max_purity_at_leaf))")
+		# 	elseif (min_purity_increase >= min_purity_increase_thresh)
+		# 		println("Warning! It is advised to use max_purity_at_leaf<$(min_purity_increase_thresh) with loss $(loss_function)"
+		# 			* "(given $(min_purity_increase))")
+		# end
 		elseif loss_function in [util.gini, util.zero_one] && (max_purity_at_leaf > 1.0 || max_purity_at_leaf <= 0.0)
 			throw_n_log("max_purity_at_leaf for loss $(loss_function) must be in (0,1]"
 				* "(given $(max_purity_at_leaf))")
@@ -557,6 +578,100 @@
 
 	end
 
+	# function optimize_tree_parameters!(
+	# 		X               :: OntologicalDataset{T, N},
+	# 		initCondition   :: DecisionTree._initCondition,
+	# 		allowRelationGlob :: Bool,
+	# 		test_operators  :: AbstractVector{<:TestOperator}
+	# 	) where {T, N}
+
+	# 	# Adimensional ontological datasets:
+	# 	#  flatten to adimensional case + strip of all relations from the ontology
+	# 	if prod(channel_size(X)) == 1
+	# 		if (length(ontology(X).relationSet) > 0)
+	# 			warn("The OntologicalDataset provided has degenerate channel_size $(channel_size(X)), and more than 0 relations: $(ontology(X).relationSet).")
+	# 		end
+	# 		# X = OntologicalDataset{T, 0}(ModalLogic.strip_ontology(ontology(X)), @views ModalLogic.strip_domain(domain(X)))
+	# 	end
+
+	# 	ontology_relations = deepcopy(ontology(X).relationSet)
+
+	# 	# Fix test_operators order
+	# 	test_operators = unique(test_operators)
+	# 	ModalLogic.sort_test_operators!(test_operators)
+		
+	# 	# Adimensional operators:
+	# 	#  in the adimensional case, some pairs of operators (e.g. <= and >)
+	# 	#  are complementary, and thus it is redundant to check both at the same node.
+	# 	#  We avoid this by only keeping one of the two operators.
+	# 	if prod(channel_size(X)) == 1
+	# 		# No ontological relation
+	# 		ontology_relations = []
+	# 		if test_operators ⊆ ModalLogic.all_lowlevel_test_operators
+	# 			test_operators = [TestOpGeq]
+	# 			# test_operators = filter(e->e ≠ TestOpGeq,test_operators)
+	# 		else
+	# 			warn("Test operators set includes non-lowlevel test operators. Update this part of the code accordingly.")
+	# 		end
+	# 	end
+
+	# 	# Softened operators:
+	# 	#  when the biggest world only has a few values, softened operators fallback
+	# 	#  to being hard operators
+	# 	# max_world_wratio = 1/prod(max_channel_size(X))
+	# 	# if TestOpGeq in test_operators
+	# 	# 	test_operators = filter((e)->(typeof(e) != _TestOpGeqSoft || e.alpha < 1-max_world_wratio), test_operators)
+	# 	# end
+	# 	# if TestOpLeq in test_operators
+	# 	# 	test_operators = filter((e)->(typeof(e) != _TestOpLeqSoft || e.alpha < 1-max_world_wratio), test_operators)
+	# 	# end
+
+
+	# 	# Binary relations (= unary modal operators)
+	# 	# Note: the identity relation is the first, and it is the one representing
+	# 	#  propositional splits.
+		
+	# 	if RelationId in ontology_relations
+	# 		throw_n_log("Found RelationId in ontology provided. No need.")
+	# 		# ontology_relations = filter(e->e ≠ RelationId, ontology_relations)
+	# 	end
+
+	# 	if RelationGlob in ontology_relations
+	# 		throw_n_log("Found RelationGlob in ontology provided. Use allowRelationGlob = true instead.")
+	# 		# ontology_relations = filter(e->e ≠ RelationGlob, ontology_relations)
+	# 		# allowRelationGlob = true
+	# 	end
+
+	# 	relationSet = [RelationId, RelationGlob, ontology_relations...]
+	# 	relationId_id = 1
+	# 	relationGlob_id = 2
+	# 	ontology_relation_ids = map((x)->x+2, 1:length(ontology_relations))
+
+	# 	needToComputeRelationGlob = (allowRelationGlob || (initCondition == startWithRelationGlob))
+
+	# 	# Modal relations to compute gammas for
+	# 	inUseRelation_ids = if needToComputeRelationGlob
+	# 		[relationGlob_id, ontology_relation_ids...]
+	# 	else
+	# 		ontology_relation_ids
+	# 	end
+
+	# 	# Relations to use at each split
+	# 	availableRelation_ids = []
+
+	# 	push!(availableRelation_ids, relationId_id)
+	# 	if allowRelationGlob
+	# 		push!(availableRelation_ids, relationGlob_id)
+	# 	end
+
+	# 	availableRelation_ids = [availableRelation_ids..., ontology_relation_ids...]
+
+	# 	(
+	# 		test_operators, relationSet,
+	# 		relationId_id, relationGlob_id,
+	# 		inUseRelation_ids, availableRelation_ids
+	# 	)
+	# end
 
 	function _fit(
 			Xs                      :: MultiFrameModalDataset,
@@ -564,6 +679,7 @@
 			W                       :: AbstractVector{U},
 			##########################################################################
 			loss_function           :: Function,
+			n_classes               :: Int,
 			max_depth               :: Int,
 			min_samples_leaf        :: Int, # TODO generalize to min_samples_leaf_relative and min_weight_leaf
 			min_purity_increase     :: AbstractFloat,
@@ -607,7 +723,7 @@
 		# Process stack of nodes
 		stack = NodeMeta{Float64}[root]
 		currently_processed_nodes::Vector{NodeMeta{Float64}} = []
-		writing_lock = Threads.Condition()
+		# writing_lock = Threads.Condition()
 		@inbounds while length(stack) > 0
 			rngs = [DecisionTree.spawn_rng(rng) for _n in 1:length(stack)]
 			# Pop nodes and queue them to be processed
@@ -634,10 +750,11 @@
 					allowRelationGlob,
 					######################################################################
 					indX,
+					n_classes,
 					######################################################################
 					_perform_consistency_check,
 					######################################################################
-					writing_lock,
+					# writing_lock,
 					######################################################################
 					rngs[i_node]
 				)
@@ -688,10 +805,10 @@
 			perform_consistency_check :: Bool,
 			##########################################################################
 			rng = Random.GLOBAL_RNG :: Random.AbstractRNG
-		) where {S<:Float64, U}
+		) where {S<:String, U}
 
 		if isnothing(loss_function)
-			loss_function = util.variance
+			loss_function = util.entropy
 		end
 		
 		# Check validity of the input
@@ -712,8 +829,12 @@
 			allowRelationGlob,
 		)
 
-		Y_ = Y
-		
+		# Transform labels to categorical form
+		labels, Y_ = util.assign(Y)
+		# print(labels, Y_)
+		# println("countmap: ")
+		# println(StatsBase.countmap(Y_))
+
 		# Call core learning function
 		root, indX = _fit(
 				Xs,
@@ -721,6 +842,7 @@
 				W,
 				########################################################################
 				loss_function,
+				length(labels),
 				max_depth,
 				min_samples_leaf,
 				min_purity_increase,
@@ -737,6 +859,6 @@
 		)
 		
 		# Create tree with labels and categorical leaves
-		return Tree{S}(root, indX, initConditions)
+		return Tree{S}(root, labels, indX, initConditions)
 	end
 end
